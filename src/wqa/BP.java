package wqa;

public class BP {
	private double[] hide1_x;//// 输入层即第一层隐含层的输入；hide1_x[数据的特征数目+1]， hide1_x[0]为1
	private double[][] hide1_w;// 隐含层权值，hide1_w[本层的节点的数目][数据的特征数目+1];hide_w[0][0]为偏置量
	private double[] hide1_errors;// 隐含层的误差,hide1_errors[节点个数]

	private double[] out_x;// 输出层的输入值即第二次层隐含层的输出 out_x[上一层的节点数目+1]， out_x[0]为1
	private double[][] out_w;// 输出层的权值 hide1_w[节点的数目][上一层的节点数目+1]//
								// out_w[0][0]为偏置量
	private double[] out_errors;// 输出层的误差 hide1_errors[节点个数]

	private double[] target;// 目标值，target[输出层的节点个数]

	private double rate;// 学习速率

	public static double[] errors = new double[300000];//统计每次训练后的数值差值:目标值-输出层输出值
	public static int errors_index;
	public static int errors_counts;
	
	public BP(int input_node, int hide1_node, int out_node, double rate) {
		super();
		//input_node 输入层节点数
		//hide1_node 第一层隐含层节点数
		//out_node 输出层节点数
		
		// 输入层即第一层隐含层的输入
		hide1_x = new double[input_node + 1];

		// 第一层隐含层
		hide1_w = new double[hide1_node][input_node + 1];
		hide1_errors = new double[hide1_node];

		// 输出层
		out_x = new double[hide1_node + 1];
		out_w = new double[out_node][hide1_node + 1];
		out_errors = new double[out_node];

		target = new double[out_node];

		// 学习速率
		this.rate = rate;
		init_weight();// 1.初始化网络的权值
		
		this.errors_index = 0;
		this.errors_counts = 1;
	}

	/**
	 * 初始化权值
	 */
	public void init_weight() {

		set_weight_hide(hide1_w);
		set_weight_out(out_w);
	}

	/**
	 * 初始化权值
	 * 
	 * @param w
	 */
	private void set_weight_hide(double[][] w) {
		for (int i = 0, len = w.length; i != len; i++)
			for (int j = 0, len2 = w[i].length; j != len2; j++) {
				double temp = Math.random();
				//in
				//w[i][j] = 0.5 + (temp >= 0.5 ? 0.5 - temp : temp) / 10;
				//out
				w[i][j] = 0.7 + (temp >= 0.5 ? 0.5 - temp : temp) / 10;
			}
	}
	private void set_weight_out(double[][] w) {
		for (int i = 0, len = w.length; i != len; i++)
			for (int j = 0, len2 = w[i].length; j != len2; j++) {
				double temp = Math.random();
				//in
				//w[i][j] = -1 + (temp >= 0.5 ? 0.5 - temp : temp)/10;
				//out
				w[i][j] = 0.75 + (temp >= 0.5 ? 0.5 - temp : temp)/10;
			}
	}
	/**
	 * 2.训练数据集
	 * 
	 * @param TrainData
	 *            训练数据
	 * @param target
	 *            目标
	 */
	public void train(double[] TrainData, double[] target) {
		// 2.1导入训练数据集和目标值
		setHide1_x(TrainData);
		setTarget(target);

		// 2.2：向前传播得到输出值；
		double[] output = new double[out_w.length + 1];
		forword(hide1_x, output);
		System.out.println("输入值："+TrainData[0]+" "+TrainData[1]+" "+TrainData[2]
				+" 目标值："+target[0]
				+" 输出层输出值"+output[1]
				+" 差值【"+(target[0]-output[1])*10000+"】"
				+"【"+(target[0]-output[1])/target[0]+"】");
		//if((target[0]-output[1])/target[0] <= 0.004 )		{this.printAll();}
		if(this.errors_counts%Test.times==0){
			this.errors[this.errors_index++] = (target[0]-output[1])*10000;
		}
		// 2.3、反向传播：
		backpropagation(output);

	}

	/**
	 * 获取原始数据
	 * 
	 * @param Data
	 *            原始数据矩阵
	 */
	private void setHide1_x(double[] Data) {
		if (Data.length != hide1_x.length - 1) {
			throw new IllegalArgumentException("数据大小与输入层节点不匹配");
		}
		System.arraycopy(Data, 0, hide1_x, 1, Data.length);
		hide1_x[0] = 1.0;
	}

	/**
	 * @param target
	 *            the target to set
	 */
	private void setTarget(double[] target) {
		this.target = target;
	}

	/**
	 * 向前传播
	 * 
	 * @param x
	 *            输入值
	 * @param output
	 *            输出值
	 */
	public void forword(double[] x, double[] output) {

		// 2.2.1、获取隐含层的输出
		get_net_out(x, hide1_w, out_x);
		// 2.2.2、获取输出层的输出
		get_net_out(out_x, out_w, output);

	}

	/**
	 * 获取网络层的输出
	 * 
	 * @param x
	 *            输入矩阵
	 * @param w
	 *            权值矩阵
	 * @param net_out
	 *            接收网络层的输出数组
	 */
	private void get_net_out(double[] x, double[][] w, double[] net_out) {

		net_out[0] = 1d;
		for (int i = 0; i < w.length; i++) {
			net_out[i + 1] = get_node_put(x, w[i]);
		}

	}

	/**
	 * 获取单个节点的输出
	 * 
	 * @param x
	 *            输入矩阵
	 * @param w
	 *            权值
	 * @return 输出值
	 */
	private double get_node_put(double[] x, double[] w) {
		double z = 0d;

		for (int i = 0; i < x.length; i++) {
			z += x[i] * w[i];
		}
		// 2.激励函数
		return 1d / (1d + Math.exp(-z));
	}

	/**
	 * 反向传播过程
	 * 
	 * @param output
	 *            预测结果
	 */
	public void backpropagation(double[] output) {

		// 2.3.1、获取输出层的误差；
		get_out_error(output, target, out_errors);
		// 2.3.2、获取隐含层的误差；
		get_hide_error(out_errors, out_w, out_x, hide1_errors);
		//// 2.3.3、更新隐含层的权值；
		update_weight(hide1_errors, hide1_w, hide1_x);
		// * 2.3.4、更新输出层的权值；
		update_weight(out_errors, out_w, out_x);
	}

	/**
	 * 获取输出层的误差
	 * 
	 * @param output
	 *            预测输出值
	 * @param target
	 *            目标值
	 * @param out_error
	 *            输出层的误差
	 */
	public void get_out_error(double[] output, double[] target, double[] out_error) {
		for (int i = 0; i < target.length; i++) {
			out_error[i] = (target[i] - output[i + 1]) * output[i + 1] * (1d - output[i + 1]);
		}

	}

	/**
	 * 获取隐含层的误差
	 * 
	 * @param NeLaErr
	 *            下一层的误差
	 * @param Nextw
	 *            下一层的权值
	 * @param output
	 *            下一层的输入
	 * @param error
	 *            本层误差数组
	 */
	public void get_hide_error(double[] NeLaErr, double[][] Nextw, double[] output, double[] error) {

		for (int k = 0; k < error.length; k++) {
			double sum = 0;
			for (int j = 0; j < Nextw.length; j++) {
				sum += Nextw[j][k + 1] * NeLaErr[j];
			}
			error[k] = sum * output[k + 1] * (1d - output[k + 1]);
		}
	}

	public void update_weight(double[] err, double[][] w, double[] x) {

		double newweight = 0.0;
		for (int i = 0; i < w.length; i++) {
			for (int j = 0; j < w[i].length; j++) {
				newweight = rate * err[i] * x[j];
				w[i][j] = w[i][j] + newweight;
			}

		}
	}

	/**
	 * 预测
	 * 
	 * @param data
	 *            预测数据
	 * @param output
	 *            输出值
	 */
	public void predict(double[] data, double[] output) {

		double[] out_y = new double[out_w.length + 1];
		setHide1_x(data);
		forword(hide1_x, out_y);
		System.arraycopy(out_y, 1, output, 0, output.length);

	}

	public void printAll() {
		System.out.print("==输入层==> [");
		for(int i=0;i<hide1_x.length;i++){
			System.out.print((i>0?" ":"")+hide1_x[i]);
		} System.out.println("]");
		

		for(int i=0;i<hide1_w.length;i++){
			System.out.print("==隐含层权值==> [");
			for(int j=0;j<hide1_w[0].length;j++){
			System.out.print((j>0?" ":"")+hide1_w[i][j]);
			}System.out.println("]");
		} 
		
		System.out.print("==隐含层的误差==> [");
		for(int i=0;i<hide1_errors.length;i++){
			System.out.print((i>0?" ":"")+hide1_errors[i]);
		} System.out.println("]");
		
		System.out.print("==输出层的输入值==> [");
		for(int i=0;i<out_x.length;i++){
			System.out.print((i>0?" ":"")+out_x[i]);
		} System.out.println("]");
		
		for(int i=0;i<out_w.length;i++){
			System.out.print("==输出层的权值==> [");
			for(int j=0;j<out_w[0].length;j++){
			System.out.print((j>0?" ":"")+out_w[i][j]);
			}System.out.println("]");
		} 
		
		System.out.print("==输出层的误差==> [");
		for(int i=0;i<out_errors.length;i++){
			System.out.print((i>0?" ":"")+out_errors[i]);
		} System.out.println("]");
		
		System.out.println("学习速率："+rate);
	}
}
